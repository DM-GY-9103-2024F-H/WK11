{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week H\n",
    "\n",
    "More Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data_utils import display_confusion_matrix, object_from_json_url\n",
    "from data_utils import LFWUtils, StandardScaler\n",
    "from image_utils import make_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Tensors and Why They're Awesome\n",
    "\n",
    "Multi-dimensional slicing is definitely a nice property of tensors, but what really sets them apart is their ability to keep track of all the operations performed on them using _computational graphs_.\n",
    "\n",
    "If we define a tensor and set its `requires_grad` parameter to `True` we unlock some really nice properties that we can use for training neural networks.\n",
    "\n",
    "One of these properties is the ability to automatically calculate derivatives (OMG, calculus!) of functions defined in terms of our tensor.\n",
    "\n",
    "Let's investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Calculus and Free Derivatives\n",
    "\n",
    "Let's pretend we have the following function:\n",
    "\n",
    "$f(x) = x^4 - 0.7x^3 - 2x^2 + x + 1$\n",
    "\n",
    "And we want to find out when the function achieves its maximum and minimum values, when it equals $0$, or when it equals $0.5$.\n",
    "\n",
    "We can plot it, and easily approximate those values visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks(x):\n",
    "  return x**4 - 0.7*x**3 - 2*x**2 + x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linspace is range()'s cousin, but for floats \n",
    "#   and where the 3rd argument specifies number of steps, not length of steps\n",
    "\n",
    "x = torch.linspace(-1.3, 1.6, 300)\n",
    "y = peaks(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot([-1.3, 1.6], [0,0], '-')\n",
    "plt.plot([-1.3, 1.6], [0.5, 0.5], '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like local minimum and maximum values are approximately:\n",
    "- $x = -0.9$ (global minimum)\n",
    "- $x = 0.2$ (global maximum)\n",
    "- $x = 1.2$ (local minimum)\n",
    "\n",
    "It crosses $y = 0$ at:\n",
    "- $x = -1.2$\n",
    "- $x = -0.6$\n",
    "\n",
    "And, it crosses $y=0.5$ a bunch of times, so we'll look at that later.\n",
    "\n",
    "We can calculate exact values for these points in our graph if we define $x$ and $y$ as tensors and enable their `auto_grad` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.linspace(-1.3, 1.6, 8000, requires_grad=True)\n",
    "yt = peaks(xt)\n",
    "yt.backward(torch.ones_like(xt))\n",
    "\n",
    "dydx = xt.grad\n",
    "print(\"derivatives:\", dydx[:5])\n",
    "\n",
    "minmax_idx = (dydx.abs() < 9e-4)\n",
    "minmax_y = yt[minmax_idx]\n",
    "minmax_x = xt[minmax_idx]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(minmax_x.tolist(), minmax_y.tolist(), 'o')\n",
    "plt.show()\n",
    "\n",
    "print(\"min/max:\", minmax_x, minmax_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.linspace(-1.3, 1.6, 8000, requires_grad=True)\n",
    "yt = peaks(xt).pow(2)\n",
    "yt.backward(torch.ones_like(xt))\n",
    "\n",
    "dydx = xt.grad\n",
    "print(\"derivatives:\", dydx[:5])\n",
    "\n",
    "zeros_idx = ((dydx.abs() < 0.005) & (yt < 1e-7))\n",
    "zeros_x = xt[zeros_idx]\n",
    "zeros_y = yt[zeros_idx]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(zeros_x.tolist(), zeros_y.tolist(), 'o')\n",
    "plt.show()\n",
    "\n",
    "print(\"zeros:\", zeros_x, zeros_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.linspace(-1.3, 1.6, 8000, requires_grad=True)\n",
    "yt = peaks(xt)\n",
    "yt2 = yt.subtract(0.5).pow(2)\n",
    "yt2.backward(torch.ones_like(xt))\n",
    "\n",
    "dydx = xt.grad\n",
    "print(\"derivatives:\", dydx[:5])\n",
    "\n",
    "y05_idx = ((dydx.abs() < 0.005) & (yt2 < 2e-7))\n",
    "y05_x = xt[y05_idx]\n",
    "y05_y = yt[y05_idx]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(y05_x.tolist(), y05_y.tolist(), 'o')\n",
    "plt.show()\n",
    "\n",
    "print(\"y=0.5:\", y05_x, y05_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOTHER WAY of doing it\n",
    "\n",
    "Very expensive to calculate function over large range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "x_ = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "y_ = peaks(x_)\n",
    "xs.append(x_.item())\n",
    "ys.append(y_.item())\n",
    "\n",
    "y_.backward()\n",
    "print(x_, y_, x_.grad)\n",
    "\n",
    "x_ = x_ + 0.1 * x_.grad\n",
    "x_.retain_grad()\n",
    "\n",
    "y_ = peaks(x_)\n",
    "xs.append(x_.item())\n",
    "ys.append(y_.item())\n",
    "\n",
    "# TODO: more steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.scatter(xs, ys, marker='o', s=14, c='r')\n",
    "plt.show()\n",
    "x_.item(), y_.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, so what ?\n",
    "\n",
    "Well, now we have most of the ingredients for using a neural network to build a linear regression model using data.\n",
    "\n",
    "Let's load the housing prices dataset from `HW03`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location of the json file here\n",
    "HOUSES_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/LA_housing.json\"\n",
    "\n",
    "houses_info = object_from_json_url(HOUSES_FILE)\n",
    "\n",
    "houses_raw_df = pd.DataFrame.from_records(houses_info)\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "houses_df = std_scaler.fit_transform(houses_raw_df)\n",
    "\n",
    "houses_train, houses_test = train_test_split(houses_df, test_size=0.2)\n",
    "\n",
    "houses_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = houses_train.drop(columns=[\"value\"])\n",
    "train_values = houses_train[\"value\"]\n",
    "\n",
    "x_train = torch.tensor(train_features.values, dtype=torch.float32)\n",
    "Y_train = torch.tensor(train_values.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = houses_test.drop(columns=[\"value\"])\n",
    "test_values = houses_test[\"value\"]\n",
    "\n",
    "x_test = torch.tensor(test_features.values, dtype=torch.float32)\n",
    "Y_test = torch.tensor(test_values.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "model = nn.Linear(len(houses_df.columns) - 1, 1)\n",
    "\n",
    "for c in range(32):\n",
    "    Y_ = model(x_train)\n",
    "    loss = (Y_ - Y_train).pow(2).mean().pow(0.5)\n",
    "    loss.backward()\n",
    "    if c % 4 == 0:\n",
    "      print(c, loss.item())\n",
    "\n",
    "    for p in model.parameters():\n",
    "      p.data.sub_(p.grad.data * learning_rate)\n",
    "      p.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_std = pd.Series(model(x_train).detach().numpy().squeeze(), name=\"value\")\n",
    "\n",
    "std_scaler.inverse_transform(Y_std[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "model =  nn.Sequential(\n",
    "  nn.Linear(len(houses_df.columns) - 1, len(houses_df.columns) - 1),\n",
    "  nn.ReLU(),\n",
    "  # TODO: add layers\n",
    "  nn.Linear(len(houses_df.columns) - 1, 1),\n",
    ")\n",
    "\n",
    "for c in range(32):\n",
    "    Y_ = model(x_train)\n",
    "    loss = (Y_ - Y_train).pow(2).mean().pow(0.5)\n",
    "    loss.backward()\n",
    "    if c % 4 == 0:\n",
    "      print(c, loss.item())\n",
    "\n",
    "    for p in model.parameters():\n",
    "      p.data.sub_(p.grad.data * learning_rate)\n",
    "      p.grad.zero_()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  Y_ = model(x_test)\n",
    "  loss = (Y_ - Y_test).pow(2).mean().pow(0.5)\n",
    "  print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "model =  nn.Sequential(\n",
    "  nn.Linear(len(houses_df.columns) - 1, len(houses_df.columns) - 1),\n",
    "  nn.ReLU(),\n",
    "  # TODO: add layers\n",
    "  nn.Linear(len(houses_df.columns) - 1, 1),\n",
    ")\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "for c in range(32):\n",
    "    optim.zero_grad()\n",
    "    Y_ = model(x_train)\n",
    "    loss = (Y_ - Y_train).pow(2).mean().pow(0.5)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if c % 4 == 0:\n",
    "      print(c, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  Y_ = model(x_test)\n",
    "  loss = (Y_ - Y_test).pow(2).mean().pow(0.5)\n",
    "  print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images?\n",
    "\n",
    "Load the Labeled Faces in the Wild dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Split data into train/test and do any pre-processing\n",
    "\n",
    "# 1. Dataloader? Can computer handle entire dataset at once?\n",
    "\n",
    "# 2. What's my architecture/model?\n",
    "\n",
    "# 3. What's my cost/loss function?\n",
    "\n",
    "# 4. What's my optimizer?\n",
    "\n",
    "# 5. What's my evaluation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.333)\n",
    "\n",
    "x_train = torch.tensor(train[\"pixels\"], dtype=torch.float32)\n",
    "Y_train = torch.tensor(train[\"labels\"])\n",
    "\n",
    "x_test = torch.tensor(test[\"pixels\"], dtype=torch.float32)\n",
    "Y_test = torch.tensor(test[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, len(train[\"pixels\"]), 100):\n",
    "  display(make_image(train[\"pixels\"][idx], 130))\n",
    "  print(train[\"labels\"][idx], LFWUtils.LABELS[train[\"labels\"][idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "- batch\n",
    "- random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "  def __init__(self, imgs, labels):\n",
    "    self.imgs = imgs\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.imgs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(FaceDataset(x_train, Y_train), batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(FaceDataset(x_test, Y_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(model, data):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    csum = 0\n",
    "    for x, Y in data:\n",
    "      Y_ = model(x).argmax(dim=1)\n",
    "      csum += (Y_ == Y).sum().item()\n",
    "    return csum / len(data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "model = nn.Linear(x_train.shape[1], len(Y_train.unique()))\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  for x, Y in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    Y_ = model(x)\n",
    "    loss = loss_fn(Y_, Y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 2 == 0:\n",
    "    train_acc = calc_accuracy(model, train_dataloader)\n",
    "    test_acc = calc_accuracy(model, test_dataloader)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_accuracy(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(model, data):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    data_labels = []\n",
    "    pred_labels = []\n",
    "    for x, Y in data:\n",
    "      Y_ = model(x).argmax(dim=1)\n",
    "      data_labels += [l.item() for l in Y]\n",
    "      pred_labels += [l.item() for l in Y_]\n",
    "    return data_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_pred_labels = get_labels(model, train_dataloader)\n",
    "test_labels, test_pred_labels = get_labels(model, test_dataloader)\n",
    "\n",
    "display_confusion_matrix(train_labels, train_pred_labels, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_pred_labels, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6\n",
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.2),\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 8),\n",
    "  nn.ReLU(),\n",
    "  nn.Dropout(0.2),\n",
    "  nn.Linear(x_train.shape[1] // 8, len(Y_train.unique())),\n",
    ")\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for x, Y in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    Y_ = model(x)\n",
    "    loss = loss_fn(Y_, Y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 2 == 0:\n",
    "    train_acc = calc_accuracy(model, train_dataloader)\n",
    "    test_acc = calc_accuracy(model, test_dataloader)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_accuracy(model, train_dataloader), calc_accuracy(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_pred_labels = get_labels(model, train_dataloader)\n",
    "test_labels, test_pred_labels = get_labels(model, test_dataloader)\n",
    "\n",
    "display_confusion_matrix(train_labels, train_pred_labels, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_pred_labels, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFWUtils.top_precision(test_labels, test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFWUtils.top_recall(test_labels, test_pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
