{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week H\n",
    "\n",
    "More Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from data_utils import display_confusion_matrix, object_from_json_url\n",
    "from data_utils import LFWUtils, StandardScaler\n",
    "from image_utils import make_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Tensors and Why They're Awesome\n",
    "\n",
    "Multi-dimensional slicing is definitely a nice property of tensors, but what really sets them apart is their ability to keep track of all the operations performed on them using _computational graphs_.\n",
    "\n",
    "If we define a tensor and set its `requires_grad` parameter to `True` we unlock some really nice properties that we can use for training neural networks.\n",
    "\n",
    "One of these properties is the ability to automatically calculate derivatives (OMG, calculus!) of functions defined in terms of our tensor.\n",
    "\n",
    "Let's investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Calculus and Free Derivatives\n",
    "\n",
    "Let's pretend we have the following function:\n",
    "\n",
    "$f(x) = x^4 - 0.7x^3 - 2x^2 + x + 1$\n",
    "\n",
    "And we want to find out when the function achieves its maximum and minimum values, when it equals $0$, or when it equals $0.5$.\n",
    "\n",
    "We can plot it, and easily approximate those values visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaks(x):\n",
    "  return x**4 - 0.7*x**3 - 2*x**2 + x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linspace is range()'s cousin, but for floats \n",
    "#   and where the 3rd argument specifies number of steps, not length of steps\n",
    "\n",
    "x = torch.linspace(-1.3, 1.6, 300)\n",
    "y = peaks(x)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot([-1.3, 1.6], [0,0], '-')\n",
    "plt.plot([-1.3, 1.6], [0.5, 0.5], '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like local minimum and maximum values are approximately:\n",
    "- $x = -0.9$ (global minimum)\n",
    "- $x = 0.2$ (global maximum)\n",
    "- $x = 1.2$ (local minimum)\n",
    "\n",
    "It crosses $y = 0$ at:\n",
    "- $x = -1.2$\n",
    "- $x = -0.6$\n",
    "\n",
    "And, it crosses $y=0.5$ a bunch of times, so we'll look at that later.\n",
    "\n",
    "We can calculate exact values for these points in our graph if we define $x$ and $y$ as tensors and enable their `auto_grad` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.linspace(-1.3, 1.6, 8000, requires_grad=True)\n",
    "yt = peaks(xt)\n",
    "yt.backward(torch.ones_like(xt))\n",
    "\n",
    "dydx = xt.grad\n",
    "print(\"derivatives:\", dydx[:5])\n",
    "\n",
    "minmax_idx = (dydx.abs() < 9e-4)\n",
    "minmax_y = yt[minmax_idx]\n",
    "minmax_x = xt[minmax_idx]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(minmax_x.tolist(), minmax_y.tolist(), 'o')\n",
    "plt.show()\n",
    "\n",
    "print(\"min/max:\", minmax_x, minmax_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait. What?\n",
    "\n",
    "Let's look at the individual commands at the cell above.\n",
    "\n",
    "`xt`: this is a $1D$ tensor of shape $8000$ with value from $-1.3$ to $1.6$.\n",
    "\n",
    "`yt`: this is a $1D$ tensor of shape $8000$ which holds the results of calling `peaks()` on every value of `xt`.\n",
    "\n",
    "`yt.backwards(torch.ones_like(xt))`: this calculates the derivatives (slope) of the equation `peak()` for every point of `yt` and `xt`. The `torch.ones_like(xt)` parameter is a bit unconventional and usually we'll just call `backwards()` without any parameters. It's necessary here because instead of asking for the derivative of an equation at one specific point, we want to get the derivatives for all points in our `xt` range tensor.\n",
    "\n",
    "`dydx = xt.grad`: after calling `backward()` on a tensor (`yt`) that depends on tensors with `requires_grad` (`xt`), the tensors with `requires_grad` will have their gradients/slope store in the `grad` member variable.\n",
    "\n",
    "`minmax_idx = (dydx.abs() < 9e-4)`: since our function is being evaluated on a discrete set of values inside `xt`, we might not have the exact `xt` that gives an exact slope of $0$, so `dydx.abs() < 9e-4` is a boolean indexing of all values of dydx that are really close to $0$.\n",
    "\n",
    "`minmax_y = yt[minmax_idx]` and `minmax_x = xt[minmax_idx]`: this gets the actual `x` and `y` values where the slope of `peaks()` is really really close to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Zero\n",
    "\n",
    "We found `x` and `y` values for when our `peaks()` function is at its `max` and `min` values.\n",
    "\n",
    "If we want to find when our function is $0$ we can use a little trick and just square it. This will turn any $0$ crossing into a min, and we can repeat the same process as above.\n",
    "\n",
    "`yt = peaks(xt).pow(2)`: this squares our function, so _y-axis_ crossings become minimum values.\n",
    "\n",
    "`zeros_idx = ((dydx.abs() < 0.005) & (yt < 1e-7))`: we add an extra condition to the boolean index, so we only plot the minimum values where the derivate is $0$ and `yt` is close to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.linspace(-1.3, 1.6, 8000, requires_grad=True)\n",
    "yt = peaks(xt).pow(2)\n",
    "yt.backward(torch.ones_like(xt))\n",
    "\n",
    "dydx = xt.grad\n",
    "print(\"derivatives:\", dydx[:5])\n",
    "\n",
    "zeros_idx = ((dydx.abs() < 0.005) & (yt < 1e-7))\n",
    "zeros_x = xt[zeros_idx]\n",
    "zeros_y = yt[zeros_idx]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(zeros_x.tolist(), zeros_y.tolist(), 'o')\n",
    "plt.show()\n",
    "\n",
    "print(\"zeros:\", zeros_x, zeros_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding other values\n",
    "\n",
    "If we want to find what values of `xt` give a specific value for `yt` we can use a similar trick.\n",
    "\n",
    "We shift the function up or down to make that `yt` value become $0$, then square the function and repeat the steps as above.\n",
    "\n",
    "For example, to find values of `xt` that make `peaks()` equal to $0.5$, we subtract $0.5$ and square `peaks()`.\n",
    "\n",
    "`yt2 = yt.subtract(0.5).pow(2)`: this is the function we use to take the derivative now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.linspace(-1.3, 1.6, 8000, requires_grad=True)\n",
    "yt = peaks(xt)\n",
    "yt2 = yt.subtract(0.5).pow(2)\n",
    "yt2.backward(torch.ones_like(xt))\n",
    "\n",
    "dydx = xt.grad\n",
    "print(\"derivatives:\", dydx[:5])\n",
    "\n",
    "y05_idx = ((dydx.abs() < 0.005) & (yt2 < 2e-7))\n",
    "y05_x = xt[y05_idx]\n",
    "y05_y = yt[y05_idx]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(y05_x.tolist(), y05_y.tolist(), 'o')\n",
    "plt.show()\n",
    "\n",
    "print(\"y=0.5:\", y05_x, y05_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving for min/max iteratively\n",
    "\n",
    "Our `peaks()` function is pretty simple, as it only depends on one variable, `x`, and the range we're calculating it over is pretty small, $[-1.2, 1.6]$.\n",
    "\n",
    "What if our `peaks()` function was more complex and it took minutes to calculate? How can we find its `min` or `max` values?\n",
    "\n",
    "This is the more common case for `grad` and `backward()`. We evaluate a function once, at one specific input value, and calculate which direction it should move in order to increase or decrease the value of our function.\n",
    "\n",
    "We can use the `peaks()` function to illustrate. Let's calculate the value of `x` that gives the smallest value for `peaks(x)`.\n",
    "\n",
    "`xm`: this is the current guess for the value of `x` which gives the smallest value for `peaks()`. We'll initialize it at $0.15$, which is the halfway point of our `x` range.\n",
    "\n",
    "`xms` and `yms`: these will hold the progression of the `xm` and `ym` variables as they move towards their objectives.\n",
    "\n",
    "`ym`: the value of `peaks()` at the current `xm`.\n",
    "\n",
    "`backwards()`: calculate the slope of `ym` with respect to its inputs.\n",
    "\n",
    "`xm = xm + 0.1 * xm.grad`: update `xm` according to the slope of `peaks()` at `xm`. If the slope is positive, decrease `xm`, if the slope is negative, increase `xm`. This will move `x_m` towards a minimum value of `peaks()`. If we wanted to move towards a maximum value, we increase `xm` for positive slopes and decrease it for negative slopes.\n",
    "\n",
    "The $0.1$ factor determines how big our steps should be when we update `xm`. There's a tradeoff here: large steps can get to the desired value quicker, but can also totally skip the desired value and end up in some non-desired part of our equation. Small steps, on the other hand, take a little longer to find the objective, but usually converge on the correct value.\n",
    "\n",
    "`xm.retain_grad()`: we're again using tensors for educational purposes here, and accumulating gradients in a unconventional way. We have to call this to make sure we can later access the gradient of something that was itself calculated from a gradient.\n",
    "\n",
    "A tensor's `item()` member function just returns that tensor's value as a regular Python number. Similarly, if we want to get a tensor as a regular Python list we can call its `tolist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "xm = torch.tensor(0.15, requires_grad=True)\n",
    "\n",
    "ym = peaks(xm)\n",
    "ym.backward()\n",
    "print(xm.item(), ym.item(), xm.grad)\n",
    "\n",
    "xs.append(xm.item())\n",
    "ys.append(ym.item())\n",
    "\n",
    "xm = xm - 0.1 * xm.grad\n",
    "xm.retain_grad()\n",
    "\n",
    "ym = peaks(xm)\n",
    "ym.backward()\n",
    "print(xm.item(), ym.item(), xm.grad)\n",
    "\n",
    "xs.append(xm.item())\n",
    "ys.append(ym.item())\n",
    "\n",
    "# TODO: more steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X's journey\n",
    "\n",
    "We saved all of the intermediate values of `xm` and `ym` so we can plot them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y)\n",
    "plt.scatter(xs, ys, marker='o', s=14, c='r')\n",
    "plt.show()\n",
    "xs[-1], ys[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking all the steps\n",
    "\n",
    "We took one step. We could loop and take $10$ steps, or take as many steps as are necessary to get to the closest max/min value of our function.\n",
    "\n",
    "Let's add a loop to the cell above that repeats the following:\n",
    "\n",
    "- calculate `ym`\n",
    "- save `xm` and `ym`\n",
    "- calculate `gradient`\n",
    "- update `xm`\n",
    "- repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, so what ?\n",
    "\n",
    "Well, now we have the most important ingredients for building a neural network that performs regression.\n",
    "\n",
    "We know how to load data into a `DataFrame`, once we pass this data through a neural network with random values for its parameters, we can calculate the `error` of our cost function in relation to all of the parameters of the network, and then calculate which direction to move all of the parameters to decrease our error.\n",
    "\n",
    "Let's load the housing prices dataset from `HW03`.\n",
    "\n",
    "As always, we'll encode and scale our data if needed, and then we'll use the `train_test_split()` function to split our `DataFrame` into $2$ separate datasets, a training dataset with $80\\%$ of the rows, and a test dataset with $20\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the location of the json file here\n",
    "HOUSES_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/LA_housing.json\"\n",
    "\n",
    "houses_info = object_from_json_url(HOUSES_FILE)\n",
    "\n",
    "houses_raw_df = pd.DataFrame.from_records(houses_info)\n",
    "\n",
    "house_scaler = StandardScaler()\n",
    "houses_df = house_scaler.fit_transform(houses_raw_df)\n",
    "\n",
    "houses_train, houses_test = train_test_split(houses_df, test_size=0.2)\n",
    "\n",
    "houses_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features\n",
    "\n",
    "Just like with the `LinearRegression` models, we have to separate our independent features and our outcome feature.\n",
    "\n",
    "This time we put them both into tensors.\n",
    "\n",
    "The `x` tensor holds all of the independent features for all of the data points, and the `y` tensor their corresponding outcomes (prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = houses_train.drop(columns=[\"value\"])\n",
    "train_values = houses_train[\"value\"]\n",
    "\n",
    "x_train = torch.Tensor(train_features.values)\n",
    "y_train = torch.Tensor(train_values.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our model\n",
    "\n",
    "We'll use a very basic neural network model that has an input layer with a neuron for each feature, and a single output neuron for the price prediction.\n",
    "\n",
    "Something like this:\n",
    "\n",
    "# ADD IMAGE HERE\n",
    "\n",
    "Where the initial values for the model parameters are selected at random by default.\n",
    "\n",
    "We can iterate over out model's parameters and print their shapes, or calculate overall number of parameters using the `numel()` function of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(len(train_features.columns), 1)\n",
    "\n",
    "psum = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "  print(p.shape)\n",
    "  psum += p.numel()\n",
    "\n",
    "print(\"number of parameters:\", psum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training\n",
    "\n",
    "This will look similar to the iterative approach for finding the minimum value of a function we saw above.\n",
    "\n",
    "For each step of our iteration we will:\n",
    "\n",
    "- calculate a price prediction for all of the rows in our dataset\n",
    "- calculate the overall error for all of the price predictions\n",
    "- calculate the derivative of this error with respect to the model parameters\n",
    "- update model parameters to decrease error\n",
    "- repeat\n",
    "\n",
    "A few things to note about this process:\n",
    "\n",
    "1\\. We are calculating all of the predictions for all of our rows of data with a single call: `y = model(x)`. `PyTorch` models are smart and they know we want to do the same thing for all of the rows. This optimizes and parallelizes the process.\n",
    "\n",
    "2\\. The cost function (called `loss` here) is the `L2` distance between all price predictions and all actual prices in our dataset calculated in one go. It's a single number we can take the derivative of. We could skip the square root, but this way our units stay consistent and error is in standard deviations.\n",
    "\n",
    "3\\. The parameters we are optimizing and updating at each iteration aren't our features, but the weights and thresholds of each of our $6$ neurons, which have `requires_grad` turned on by default. At each step we update the model's parameters with `p.data.sub_(p.grad.data * learning_rate)`. This is the very bureaucratic form of doing something like: `p -= p * lr`. Since we are dealing with parameter tensors that keep all kinds of extra information about their values, we have to operate on their `data` members.\n",
    "\n",
    "4\\. Once we have used the parameters gradients to update our model we have to clear them by calling `grad.zero_()`. We'll see why soon, but by default if we are reusing the same tensors (in this case our model's parameters) we have to make sure they don't accumulate gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "for c in range(32):\n",
    "  y_pred = model(x_train)\n",
    "  loss = (y_pred - y_train).pow(2).mean().pow(0.5)\n",
    "  loss.backward()\n",
    "  if c % 4 == 0:\n",
    "    print(c, loss.item())\n",
    "\n",
    "  for p in model.parameters():\n",
    "    p.data.sub_(p.grad.data * learning_rate)\n",
    "    p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "What's happening in the above cell?\n",
    "\n",
    "What happens if we keep running it over and over?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the train dataset\n",
    "\n",
    "Once we're happy with the training, we can get predictions for all of our houses in dollars by running the model and reversing the scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_std = pd.DataFrame(model(x_train).tolist(), columns=[\"value\"])\n",
    "y_usd = house_scaler.inverse_transform(y_std)\n",
    "\n",
    "y_usd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing the Network\n",
    "\n",
    "The error we were getting above was around $1.0$ standard deviation. That's not bad, but it's also not good.\n",
    "\n",
    "If we want to improve our model we can try adding layers to our Neural Network. We just have to make sure we add an activation function between the neurons. These are the functions that keep our model parameters within a nice, expected, range.\n",
    "\n",
    "This is how we build the following network:\n",
    "\n",
    "# ADD IMAGE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Linear(len(train_features.columns), len(train_features.columns)),\n",
    "  nn.Sigmoid(),\n",
    "  nn.Linear(len(train_features.columns), 1),\n",
    ")\n",
    "\n",
    "# TODO: calculate the number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... many... parameters\n",
    "\n",
    "How many parameters do we have now? We might not want to keep updating them ourselves.\n",
    "\n",
    "Relying on a for loop to get all the parameters and remembering to call `grad.zero_()` at the right time is just prone to errors and inefficiencies.\n",
    "\n",
    "Luckily, `PyTorch` has some optimizers we can use. They usually take our model as an input, along with some other parameters, and give us a simpler interface to control the optimization process.\n",
    "\n",
    "### Initialize Optimizer\n",
    "\n",
    "We're going to use one of the simpler optimizers to performs [_stochastic gradient descent_](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). Gradient descent is the official name of the algorithm that calculates which way to update our parameters given the slope of our cost function. _Stochastic_ means that it will use statistics to make certain decisions that don't have exact solutions.\n",
    "\n",
    "The documentation for the [`SGD` optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) has more info about the algorithm and the parameters it takes.\n",
    "\n",
    "Other than simplifying our training code, these pre-built optimizers also perform dynamic learning rate adjustment and some other tricks that make our overall process not so sensitive to an exact learning rate.\n",
    "\n",
    "The `PyTorch` library also has a number of [other optimizers](https://pytorch.org/docs/stable/optim.html#algorithms) useful for performing gradient descent. In addition to `SGD()` we can also try [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) or [Adagrad](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it\n",
    "\n",
    "We can train our new model, just like before, except now the training loop should be a little bit simpler.\n",
    "\n",
    "We still have to call `zero_grad()`, but only on the optimizer. It will take care of clearing the gradients for each of the model's parameters for us.\n",
    "\n",
    "And, after we calculate the slope of our cost function, we call `optim.step()`, so the optimizer can update the parameters with a new slope value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(32):\n",
    "  optim.zero_grad()\n",
    "  y_pred = model(x_train)\n",
    "  loss = (y_pred - y_train).pow(2).mean().pow(0.5)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  if c % 4 == 0:\n",
    "    print(c, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset\n",
    "\n",
    "We can still adjust a lot of parameters here, but before we spend too much time on this model, let's run it on the test dataset and calculate the average loss on data that wasn't used for training to see if the model is over-fitting.\n",
    "\n",
    "We'll load the test data, run the mode and calculate loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = houses_test.drop(columns=[\"value\"])\n",
    "test_values = houses_test[\"value\"]\n",
    "\n",
    "x_test = torch.Tensor(test_features.values)\n",
    "y_test = torch.Tensor(test_values.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `no_grad()`\n",
    "\n",
    "We can tell `PyTorch` to momentarily stop calculating slopes/gradients when we are not training and just want to use the model to predict prices by using the `torch.no_grad()` function to create a block of code where model runs faster and more carefree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  y_pred = model(x_test)\n",
    "  loss = (y_pred - y_test).pow(2).mean().pow(0.5)\n",
    "  print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "This isn't bad.\n",
    "\n",
    "The absolute value of the error is kind of large, but the test dataset error is comparable to the training dataset error, which is a good indication that the model is not over-fitting.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "We can spend some time adjusting the model, adding layers, changing the optimizer, the learning rate, experimenting with the optimizer's parameters, etc.\n",
    "\n",
    "This process is usually referred to as hyperparameter tuning, since we're picking parameters that will help us calculate the parameters of our neural network.\n",
    "\n",
    "Here's a cell with all of the steps combined. We can play with the network architecture and parameters a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Model\n",
    "model =  nn.Sequential(\n",
    "  nn.Linear(len(train_features.columns), 10 * len(train_features.columns)),\n",
    "  nn.Sigmoid(),\n",
    "  # TODO: add layers\n",
    "\n",
    "  nn.Linear(10 * len(train_features.columns), 1),\n",
    ")\n",
    "# TODO: calculate the number of parameters\n",
    "\n",
    "## Define Optimizer\n",
    "learning_rate = 1e-1\n",
    "# TODO: adjust parameters, add parameters, change optimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## Load Data\n",
    "x_train = torch.Tensor(train_features.values)\n",
    "y_train = torch.Tensor(train_values.values)\n",
    "x_test = torch.Tensor(test_features.values)\n",
    "y_test = torch.Tensor(test_values.values)\n",
    "\n",
    "## Train Model\n",
    "for c in range(32):\n",
    "  optim.zero_grad()\n",
    "  y_pred = model(x_train)\n",
    "  loss = (y_pred - y_train).pow(2).mean().pow(0.5)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  if c % 4 == 0:\n",
    "    print(c, loss.item())\n",
    "\n",
    "## Evaluate Model\n",
    "with torch.no_grad():\n",
    "  y_pred = model(x_train)\n",
    "  loss_train = (y_pred - y_train).pow(2).mean().pow(0.5)\n",
    "\n",
    "  y_pred = model(x_test)\n",
    "  loss_test = (y_pred - y_test).pow(2).mean().pow(0.5)\n",
    "\n",
    "  print(\"\\ntrain loss:\", loss_train.item(), \"\\ntest loss:\", loss_test.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Our model is definitely learning. It doesn't perform very well, but that's not entirely surprising.\n",
    "\n",
    "Usually what makes the biggest difference in these kinds of models is the size of the training dataset, and our houses dataset only has $4000$ samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images?\n",
    "\n",
    "Load the Labeled Faces in the Wild dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Split data into train/test and do any pre-processing\n",
    "\n",
    "# 1. Dataloader? Can computer handle entire dataset at once?\n",
    "\n",
    "# 2. What's my architecture/model?\n",
    "\n",
    "# 3. What's my cost/loss function?\n",
    "\n",
    "# 4. What's my optimizer?\n",
    "\n",
    "# 5. What's my evaluation function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.333)\n",
    "\n",
    "x_train = torch.Tensor(train[\"pixels\"])\n",
    "y_train = torch.Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = torch.Tensor(test[\"pixels\"])\n",
    "y_test = torch.Tensor(test[\"labels\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, len(train[\"pixels\"]), 100):\n",
    "  display(make_image(train[\"pixels\"][idx], 130))\n",
    "  print(train[\"labels\"][idx], LFWUtils.LABELS[train[\"labels\"][idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "- batch\n",
    "- random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "  def __init__(self, imgs, labels):\n",
    "    self.imgs = imgs\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.imgs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(FaceDataset(x_train, y_train), batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(FaceDataset(x_test, y_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(model, data):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    data_labels = []\n",
    "    pred_labels = []\n",
    "    for x, y in data:\n",
    "      y_pred = model(x).argmax(dim=1)\n",
    "      data_labels += [l.item() for l in y]\n",
    "      pred_labels += [l.item() for l in y_pred]\n",
    "    return data_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(model, data):\n",
    "  data_labels, pred_labels = get_labels(model, data)\n",
    "  correct_predictions = []\n",
    "  for y,y_pred in zip(data_labels, pred_labels):\n",
    "    correct_predictions.append(y_pred == y)\n",
    "  return sum(correct_predictions) / len(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(x_train.shape[1], len(y_train.unique()))\n",
    "\n",
    "learning_rate = 1e-5\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  for x, y in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 0:\n",
    "    train_acc = calc_accuracy(model, train_dataloader)\n",
    "    test_acc = calc_accuracy(model, test_dataloader)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_accuracy(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(model, data):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    data_labels = []\n",
    "    pred_labels = []\n",
    "    for x, y in data:\n",
    "      y_pred = model(x).argmax(dim=1)\n",
    "      data_labels += [l.item() for l in y]\n",
    "      pred_labels += [l.item() for l in y_pred]\n",
    "    return data_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_pred_labels = get_labels(model, train_dataloader)\n",
    "test_labels, test_pred_labels = get_labels(model, test_dataloader)\n",
    "\n",
    "display_confusion_matrix(train_labels, train_pred_labels, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_pred_labels, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.2),\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 8),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Dropout(0.2),\n",
    "  nn.Linear(x_train.shape[1] // 8, len(y_train.unique())),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-5\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for x, y in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 0:\n",
    "    train_acc = calc_accuracy(model, train_dataloader)\n",
    "    test_acc = calc_accuracy(model, test_dataloader)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train acc: {train_acc:.4f}, test acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "train and test eval function diverged. but both keep decreasing. might be ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_accuracy(model, train_dataloader), calc_accuracy(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_pred_labels = get_labels(model, train_dataloader)\n",
    "test_labels, test_pred_labels = get_labels(model, test_dataloader)\n",
    "\n",
    "display_confusion_matrix(train_labels, train_pred_labels, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_pred_labels, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFWUtils.top_precision(test_labels, test_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFWUtils.top_recall(test_labels, test_pred_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
